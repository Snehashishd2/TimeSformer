{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSformer: Video Classification with Divided Space-Time Attention\n",
    "\n",
    "**Paper**: [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) \u2014 Bertasius et al., ICML 2021\n",
    "\n",
    "A from-scratch PyTorch implementation of TimeSformer \u2014 a **pure transformer** for video that extends ViT by factorizing attention into temporal and spatial components.\n",
    "\n",
    "---\n",
    "\n",
    "## Why TimeSformer?\n",
    "\n",
    "Traditional video models (C3D, I3D, SlowFast) rely on 3D convolutions. TimeSformer shows that **self-attention alone** can match or beat them \u2014 but only if you decompose the attention correctly.\n",
    "\n",
    "The paper tests **5 attention schemes** and finds that **Divided Space-Time Attention (T+S)** gives the best accuracy-efficiency tradeoff:\n",
    "\n",
    "| Scheme | How It Works | Complexity | K400 Top-1 |\n",
    "|--------|-------------|-----------|------------|\n",
    "| Space Only (S) | Attend within same frame | O(TN\u00b2) | 75.2% |\n",
    "| Joint (ST) | Attend to all patches, all frames | O((NF)\u00b2) | 77.9% |\n",
    "| **Divided (T+S)** | **Temporal then Spatial** | **O(NF(N+F))** | **78.0%** |\n",
    "| Sparse Local-Global (L+G) | Local then global subsampled | ~approx full | 75.7% |\n",
    "| Axial (T+W+H) | 3 separate 1D attentions | O(NF(T+W+H)) | 76.7% |\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Video (B, T, C, H, W)\n",
    "    \u2502\n",
    "    \u25bc\n",
    "Patch Embedding \u2500\u2500\u2500\u2500 Conv2d(3, 768, k=16, s=16)\n",
    "    \u2502\n",
    "    \u25bc\n",
    "(B, T, 196, 768)  \u2500\u2500 + time_embed + space_embed\n",
    "    \u2502\n",
    "    \u25bc\n",
    "Prepend CLS token \u2192 (B, T+1, N, D)\n",
    "    \u2502\n",
    "    \u25bc\n",
    "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
    "\u2551       \u00d712 TimeSformerBlock            \u2551\n",
    "\u2551                                       \u2551\n",
    "\u2551  1. Temporal Attn  (B*N, T, D)        \u2551\n",
    "\u2551       \u2514\u2500 + residual + LayerNorm       \u2551\n",
    "\u2551  2. Spatial Attn   (B*T, N, D)        \u2551\n",
    "\u2551       \u2514\u2500 + residual + LayerNorm       \u2551\n",
    "\u2551  3. MLP (dim\u21924*dim\u2192dim)              \u2551\n",
    "\u2551       \u2514\u2500 + residual + LayerNorm       \u2551\n",
    "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
    "    \u2502\n",
    "    \u25bc\n",
    "CLS output x[:, 0, 0] \u2192 LayerNorm \u2192 Linear \u2192 logits\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "GUGp3EDp7Oso",
    "outputId": "8aeac54f-dd0b-4e18-8e76-1573fcea7d56"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu128)\n",
      "Requirement already satisfied: trim in /usr/local/lib/python3.12/dist-packages (0.3)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu128)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision trim einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports\n",
    "\n",
    "Key dependencies:\n",
    "- **`einops`** \u2014 the `rearrange` function is the backbone of divided attention. It reshapes 4D video tensors into 3D for standard `nn.MultiheadAttention`\n",
    "- **`timm`** \u2014 provides pretrained ViT-Base weights for transfer learning (spatial attention + MLP initialized from ImageNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# import\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import timm, torchvision\n",
    "from PIL import Image"
   ],
   "metadata": {
    "id": "6W1j2NV29i7h"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XfZAvvH--fS2",
    "outputId": "2ad56d03-219e-41eb-9114-b43ecea013e3"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Video Frame Dataset\n",
    "\n",
    "Loads videos stored as folders of frame images. Uniformly samples `num_frames=8` frames using `torch.linspace`.\n",
    "\n",
    "```\n",
    "Expected directory structure:\n",
    "\n",
    "root_dir/\n",
    "\u251c\u2500\u2500 class_1/\n",
    "\u2502   \u251c\u2500\u2500 video_001/\n",
    "\u2502   \u2502   \u251c\u2500\u2500 frame_0001.jpg\n",
    "\u2502   \u2502   \u251c\u2500\u2500 frame_0002.jpg\n",
    "\u2502   \u2502   \u2514\u2500\u2500 ...\n",
    "\u2502   \u2514\u2500\u2500 video_002/\n",
    "\u2502       \u2514\u2500\u2500 ...\n",
    "\u2514\u2500\u2500 class_2/\n",
    "    \u2514\u2500\u2500 ...\n",
    "\n",
    "Output per sample: (T, C, H, W) = (8, 3, 224, 224)\n",
    "```\n",
    "\n",
    "**Uniform sampling strategy**: If the video has 120 frames and we need 8, we pick frames at indices `[0, 17, 34, 51, 68, 85, 102, 119]` \u2014 spread evenly across the full duration.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, root_dir, num_frames=8, image_size=224):\n",
    "        self.root_dir = root_dir\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
    "\n",
    "        self.samples = []\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            for video in os.listdir(cls_path):\n",
    "                video_path = os.path.join(cls_path, video)\n",
    "                self.samples.append((video_path, self.class_to_idx[cls]))\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.samples[idx]\n",
    "        frames = sorted(os.listdir(video_path))\n",
    "\n",
    "        if len(frames) < self.num_frames:\n",
    "            frames = frames * (self.num_frames // len(frames) + 1)\n",
    "\n",
    "        idxs = torch.linspace(0, len(frames) - 1, self.num_frames).long()\n",
    "        selected = [frames[i] for i in idxs]\n",
    "\n",
    "        imgs = []\n",
    "        for f in selected:\n",
    "            img = Image.open(os.path.join(video_path, f)).convert(\"RGB\")\n",
    "            imgs.append(self.transform(img))\n",
    "\n",
    "        video = torch.stack(imgs)  # (T, C, H, W)\n",
    "        return video, label"
   ],
   "metadata": {
    "id": "pywpWRO1-uWt"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. TimeSformerBlock \u2014 Divided Space-Time Attention (Core of the Paper)\n",
    "\n",
    "This is the most important cell. Each block performs attention in **two factorized steps** using `einops.rearrange` to reshape the 4D tensor `(B, T, N, D)` into 3D for standard `nn.MultiheadAttention`.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Temporal Attention \u2014 \"What happened at this position over time?\"\n",
    "\n",
    "```\n",
    "                    rearrange('b t n d -> (b n) t d')\n",
    "  (B, T, N, D)  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  (B*N, T, D)\n",
    "\n",
    "  What this does visually:\n",
    "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "  \u2502                                                              \u2502\n",
    "  \u2502   Frame 1       Frame 2       Frame 3      ...  Frame T     \u2502\n",
    "  \u2502  \u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2510     \u2502\n",
    "  \u2502  \u2502 . \u2502 . \u2502    \u2502 . \u2502 . \u2502    \u2502 . \u2502 . \u2502         \u2502 . \u2502 . \u2502     \u2502\n",
    "  \u2502  \u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524    \u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524         \u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2524     \u2502\n",
    "  \u2502  \u2502 . \u2502 Q \u2502    \u2502 . \u2502 K \u2502    \u2502 . \u2502 K \u2502         \u2502 . \u2502 K \u2502     \u2502\n",
    "  \u2502  \u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2518     \u2502\n",
    "  \u2502                                                              \u2502\n",
    "  \u2502  For EACH patch position (B*N total):                        \u2502\n",
    "  \u2502  \u2192 gather that position across ALL T frames                  \u2502\n",
    "  \u2502  \u2192 run self-attention over T tokens                          \u2502\n",
    "  \u2502  \u2192 captures MOTION / temporal dynamics at each location      \u2502\n",
    "  \u2502                                                              \u2502\n",
    "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Concrete example** (B=1, T=8, N=196, D=768):\n",
    "- Input: `(1, 8, 196, 768)` \u2192 rearrange \u2192 `(196, 8, 768)`\n",
    "- 196 independent attention operations, each over 8 temporal tokens\n",
    "- Each patch at position (row=3, col=5) attends to itself across all 8 frames\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Spatial Attention \u2014 \"What's in this frame?\"\n",
    "\n",
    "```\n",
    "                    rearrange('b t n d -> (b t) n d')\n",
    "  (B, T, N, D)  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  (B*T, N, D)\n",
    "\n",
    "  What this does visually:\n",
    "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "  \u2502                                                              \u2502\n",
    "  \u2502   Frame 1 only:                                              \u2502\n",
    "  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "  \u2502  \u2502 K\u2081  \u2502 K\u2082  \u2502 K\u2083  \u2502 K\u2084  \u2502 K\u2085  \u2502 ... \u2502K\u2081\u2089\u2086\u2502               \u2502\n",
    "  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u252c\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "  \u2502                  \u2502                                           \u2502\n",
    "  \u2502            Query patch attends to ALL                        \u2502\n",
    "  \u2502            196 patches in the SAME frame                     \u2502\n",
    "  \u2502                                                              \u2502\n",
    "  \u2502  For EACH frame instance (B*T total):                        \u2502\n",
    "  \u2502  \u2192 gather all N patches in that single frame                 \u2502\n",
    "  \u2502  \u2192 run self-attention over N tokens                          \u2502\n",
    "  \u2502  \u2192 captures APPEARANCE / spatial context                     \u2502\n",
    "  \u2502                                                              \u2502\n",
    "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Concrete example** (B=1, T=8, N=196, D=768):\n",
    "- Input: `(1, 8, 196, 768)` \u2192 rearrange \u2192 `(8, 196, 768)`\n",
    "- 8 independent attention operations, each over 196 spatial tokens\n",
    "- Each frame's patches attend to all other patches within that same frame\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Ordering Works (Temporal \u2192 Spatial)\n",
    "\n",
    "After temporal attention, each patch token is already enriched with temporal context. When spatial attention then runs, it operates on **temporally-informed** representations \u2014 so the spatial attention implicitly reasons about motion too.\n",
    "\n",
    "---\n",
    "\n",
    "### Complexity Breakdown\n",
    "\n",
    "```\n",
    "Joint attention (ST):\n",
    "  Each token attends to N*F others \u2192 O(NF) per token\n",
    "  Total: (NF+1) tokens \u00d7 (NF+1) keys \u2248 O((NF)\u00b2)\n",
    "\n",
    "Divided attention (T+S):\n",
    "  Temporal: each token attends to F others  \u2192 O(F) per token\n",
    "  Spatial:  each token attends to N others  \u2192 O(N) per token\n",
    "  Combined: O(N + F) per token\n",
    "  Total: (NF) tokens \u00d7 (N + F) keys \u2248 O(NF(N+F))\n",
    "\n",
    "With N=196, F=8:\n",
    "  Joint:   196\u00d78 = 1,568 keys per token  \u2192  1568\u00b2 \u2248 2.46M total\n",
    "  Divided: 196+8 = 204 keys per token    \u2192  1568\u00d7204 \u2248 320K total\n",
    "  \n",
    "  That's ~7.7\u00d7 fewer operations!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class TimeSformerBlock(nn.Module):\n",
    "  def __init__(self, dim, heads) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "      self.temporal_attn = nn.MultiheadAttention(dim, heads, batch_first = True)\n",
    "      self.spatial_attn = nn.MultiheadAttention(dim, heads, batch_first = True)\n",
    "\n",
    "      self.norm1 = nn.LayerNorm(dim)\n",
    "      self.norm2 = nn.LayerNorm(dim)\n",
    "      self.norm3 = nn.LayerNorm(dim)\n",
    "\n",
    "      self.mlp = nn.Sequential(\n",
    "          nn.Linear(dim, dim * 4),\n",
    "          nn.GELU(),\n",
    "          nn.Linear(dim * 4, dim),\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "      B, T, N, D  = x.shape\n",
    "\n",
    "      # Temporal Attention\n",
    "      xt = rearrange(x, 'b t n d -> (b n) t d')\n",
    "      xt = self.temporal_attn(xt, xt, xt)[0]\n",
    "      xt = rearrange(xt, '(b n) t d -> b t n d', b = B, n = N)\n",
    "      x = x + self.norm1(xt)\n",
    "\n",
    "      # Spatial Attention\n",
    "      xs = rearrange(x, 'b t n d -> (b t) n d')\n",
    "      xs = self.spatial_attn(xs, xs, xs)[0]\n",
    "      xs = rearrange(xs, '(b t) n d -> b t n d', b = B, t = T)\n",
    "      x = x + self.norm2(xs)\n",
    "\n",
    "      # MLP\n",
    "      xm = self.mlp(x)\n",
    "      x = x + self.norm3(xm)\n",
    "\n",
    "      return x"
   ],
   "metadata": {
    "id": "J-xkG7r1AFC2"
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's verify the dimensions step by step\n",
    "\n",
    "This cell traces exact tensor shapes through the block with concrete numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dimension verification \u2014 trace through TimeSformerBlock\n",
    "# Using concrete values: B=2, T=8, N=196, D=768\n",
    "\n",
    "B, T, N, D = 2, 8, 196, 768\n",
    "x = torch.randn(B, T, N, D)\n",
    "print(f\"Input shape: {x.shape}\")  # (2, 8, 196, 768)\n",
    "\n",
    "print(\"\\n--- TEMPORAL ATTENTION ---\")\n",
    "xt = rearrange(x, 'b t n d -> (b n) t d')\n",
    "print(f\"After rearrange to (B*N, T, D): {xt.shape}\")  # (392, 8, 768)\n",
    "print(f\"  \u2192 {B}*{N} = {B*N} independent sequences, each of length T={T}\")\n",
    "print(f\"  \u2192 Each sequence = one patch position tracked across {T} frames\")\n",
    "\n",
    "# After attention (shape doesn't change)\n",
    "print(f\"After MHA: {xt.shape}\")  # still (392, 8, 768)\n",
    "\n",
    "xt_back = rearrange(xt, '(b n) t d -> b t n d', b=B, n=N)\n",
    "print(f\"Rearranged back: {xt_back.shape}\")  # (2, 8, 196, 768)\n",
    "\n",
    "print(\"\\n--- SPATIAL ATTENTION ---\")\n",
    "xs = rearrange(x, 'b t n d -> (b t) n d')\n",
    "print(f\"After rearrange to (B*T, N, D): {xs.shape}\")  # (16, 196, 768)\n",
    "print(f\"  \u2192 {B}*{T} = {B*T} independent sequences, each of length N={N}\")\n",
    "print(f\"  \u2192 Each sequence = all patches within one frame\")\n",
    "\n",
    "print(f\"After MHA: {xs.shape}\")  # still (16, 196, 768)\n",
    "\n",
    "xs_back = rearrange(xs, '(b t) n d -> b t n d', b=B, t=T)\n",
    "print(f\"Rearranged back: {xs_back.shape}\")  # (2, 8, 196, 768)\n",
    "\n",
    "print(\"\\n--- SUMMARY ---\")\n",
    "print(f\"Temporal: {B*N} attention ops, seq_len={T} \u2192 captures motion\")\n",
    "print(f\"Spatial:  {B*T} attention ops, seq_len={N} \u2192 captures appearance\")\n",
    "print(f\"Output shape = Input shape = {x.shape}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. TimeSformer \u2014 Full Model with CLS Token & Positional Embeddings\n",
    "\n",
    "### The Three Learnable Parameters \u2014 Why These Shapes?\n",
    "\n",
    "```python\n",
    "self.cls_token   = nn.Parameter(torch.zeros(1, 1, 1, embed_dim))          # (1, 1, 1, D)\n",
    "self.time_embed  = nn.Parameter(torch.randn(1, num_frames + 1, embed_dim))   # (1, T+1, D)\n",
    "self.space_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))  # (1, N+1, D)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `cls_token` \u2014 shape `(1, 1, 1, D)`\n",
    "\n",
    "**What**: A learnable \"summary\" token that aggregates global video information.\n",
    "\n",
    "**Shape breakdown**:\n",
    "```\n",
    "  (  1,       1,          1,       D  )\n",
    "     \u2502        \u2502           \u2502        \u2502\n",
    "     \u2502        \u2502           \u2502        \u2514\u2500 embedding dimension (768)\n",
    "     \u2502        \u2502           \u2514\u2500 1 spatial slot (expanded to N during forward)\n",
    "     \u2502        \u2514\u2500 1 temporal slot (prepended as frame index 0)\n",
    "     \u2514\u2500 1 (broadcast across batch B via .expand())\n",
    "```\n",
    "\n",
    "**Why `zeros` not `randn`?**  \n",
    "The CLS token has no prior spatial/temporal meaning \u2014 it learns purely from data. Starting at zero ensures it doesn't inject noise before training.\n",
    "\n",
    "**In forward():**\n",
    "```python\n",
    "cls = self.cls_token.expand(B, -1, self.num_patches, -1)  # (1,1,1,D) \u2192 (B, 1, N, D)\n",
    "cls = cls + self.time_embed[:, :1, None, :]               # add time position 0\n",
    "x = torch.cat((cls, x), dim=1)                            # prepend \u2192 (B, T+1, N, D)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `time_embed` \u2014 shape `(1, T+1, D)`\n",
    "\n",
    "**What**: Tells the model *when* each frame appears in the video.\n",
    "\n",
    "**Why T+1?** The `+1` reserves position 0 for the CLS token. Actual frames use positions `1` through `T`.\n",
    "\n",
    "```\n",
    "time_embed indices:  [  0  ,   1  ,   2  ,  ...  ,   T  ]\n",
    "                        \u2502       \u2502       \u2502              \u2502\n",
    "                       CLS   frame1  frame2  ...    frameT\n",
    "```\n",
    "\n",
    "**Applied via broadcasting:**\n",
    "```python\n",
    "x = x + self.time_embed[:, 1:T+1, None, :]\n",
    "#        shape: (1, T, 1, D)\n",
    "#                         \u2191\n",
    "#                 broadcasts across N patches\n",
    "#   \u2192 SAME time encoding added to ALL patches in a given frame\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### `space_embed` \u2014 shape `(1, N+1, D)`\n",
    "\n",
    "**What**: Tells the model *where* each patch sits spatially.\n",
    "\n",
    "**Why N+1?** The `+1` accounts for the CLS token in the spatial dimension.\n",
    "\n",
    "```\n",
    "space_embed indices:  [  0  ,   1  ,   2  ,  ...  ,   N  ]\n",
    "                         \u2502       \u2502       \u2502              \u2502\n",
    "                        CLS  patch_0  patch_1  ...   patch_N-1\n",
    "```\n",
    "\n",
    "**Applied via broadcasting:**\n",
    "```python\n",
    "x = x + self.space_embed[:, None, :, :]\n",
    "#        shape: (1, 1, N+1, D)\n",
    "#                    \u2191\n",
    "#            broadcasts across T frames\n",
    "#   \u2192 SAME spatial encoding added to the same patch across ALL frames\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### The Combined Positional Identity\n",
    "\n",
    "Each token receives a **unique identity** from 3 additive sources:\n",
    "\n",
    "```\n",
    "  token[t, n] = patch_content  +  time_embed[t]  +  space_embed[n]\n",
    "                  \u2502                    \u2502                   \u2502\n",
    "                  \u2502                    \u2502                   \u2514\u2500 \"I'm at row 3, col 5\"\n",
    "                  \u2502                    \u2514\u2500 \"I'm in frame 4\"\n",
    "                  \u2514\u2500 visual features from Conv2d\n",
    "```\n",
    "\n",
    "This **factorized** approach (T+N parameters) is much cheaper than a full position matrix (T\u00d7N parameters) while still giving each token a unique space-time position.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class TimeSformer(nn.Module):\n",
    "  def __init__(self,\n",
    "               num_classes = 2,\n",
    "               num_frames = 8,\n",
    "               img_size = 224,\n",
    "               patch_size = 16,\n",
    "               embed_dim = 768,\n",
    "               depth = 12,\n",
    "               heads = 12):\n",
    "    super().__init__()\n",
    "\n",
    "    self.num_frames = num_frames\n",
    "    self.patch_size = patch_size\n",
    "    self.embed_dim = embed_dim\n",
    "    self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "    self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size = patch_size,\n",
    "                                 stride = patch_size)\n",
    "    self.cls_token = nn.Parameter(torch.zeros(1, 1, 1, embed_dim))\n",
    "\n",
    "    self.time_embed = nn.Parameter(torch.randn(1, self.num_frames + 1, embed_dim))\n",
    "    self.space_embed = nn.Parameter(torch.randn(1, 1 + self.num_patches, embed_dim))\n",
    "\n",
    "    self.blocks = nn.ModuleList(\n",
    "        TimeSformerBlock(embed_dim, heads) for _ in range(depth)\n",
    "    )\n",
    "\n",
    "    self.norm = nn.LayerNorm(embed_dim)\n",
    "    self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C, H, W = x.shape\n",
    "\n",
    "    x = x.view(B * T, C, H, W)\n",
    "    x = self.patch_embed(x)\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "    x = x.view(B, T, -1, self.embed_dim)\n",
    "\n",
    "    # Add positional embeddding exluding cls token\n",
    "    x = x + self.time_embed[:, 1:T+1, None, :] + self.space_embed[:, None, :, :]\n",
    "    # Add cls token\n",
    "    cls = self.cls_token.expand(B, -1, self.num_patches, -1)\n",
    "    cls = cls + self.time_embed[:, :1, None, :]\n",
    "    # Prepend cls token\n",
    "    x = torch.cat((cls, x), dim = 1)\n",
    "\n",
    "    # transformer blocks\n",
    "    for block in self.blocks:\n",
    "      x = block(x)\n",
    "\n",
    "    cls_out = self.norm(x[:,0,0])\n",
    "\n",
    "    return self.head(cls_out)"
   ],
   "metadata": {
    "id": "JofveBeQ_HDY"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's trace the forward pass dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Trace the forward pass of TimeSformer with concrete shapes\n",
    "# B=2, T=8, C=3, H=W=224, patch_size=16, embed_dim=768\n",
    "\n",
    "B, T, C, H, W = 2, 8, 3, 224, 224\n",
    "num_patches = (224 // 16) ** 2  # = 196\n",
    "D = 768\n",
    "\n",
    "print(\"=== Forward Pass Dimension Trace ===\")\n",
    "print(f\"\\n1. Input video: ({B}, {T}, {C}, {H}, {W})\")\n",
    "\n",
    "print(f\"\\n2. Reshape for patch_embed: ({B*T}, {C}, {H}, {W})\")\n",
    "print(f\"   \u2192 Conv2d(3, 768, k=16, s=16)\")\n",
    "print(f\"   \u2192 ({B*T}, {D}, {H//16}, {W//16})\")\n",
    "print(f\"   \u2192 flatten(2).transpose(1,2) \u2192 ({B*T}, {num_patches}, {D})\")\n",
    "print(f\"   \u2192 view back \u2192 ({B}, {T}, {num_patches}, {D})\")\n",
    "\n",
    "print(f\"\\n3. Add positional embeddings:\")\n",
    "print(f\"   time_embed[:, 1:{T}+1, None, :] shape: (1, {T}, 1, {D}) \u2192 broadcasts to (B, T, N, D)\")\n",
    "print(f\"   space_embed[:, None, :, :]       shape: (1, 1, {num_patches}, {D}) \u2192 broadcasts to (B, T, N, D)\")\n",
    "\n",
    "print(f\"\\n4. Prepend CLS token:\")\n",
    "print(f\"   cls: (1,1,1,{D}) \u2192 expand \u2192 ({B}, 1, {num_patches}, {D})\")\n",
    "print(f\"   + time_embed[:, :1, None, :] \u2192 add time pos 0\")\n",
    "print(f\"   cat(cls, x, dim=1) \u2192 ({B}, {T}+1, {num_patches}, {D}) = ({B}, {T+1}, {num_patches}, {D})\")\n",
    "\n",
    "print(f\"\\n5. Through 12 TimeSformerBlocks:\")\n",
    "print(f\"   Each block: ({B}, {T+1}, {num_patches}, {D}) \u2192 ({B}, {T+1}, {num_patches}, {D})\")\n",
    "\n",
    "print(f\"\\n6. Classification:\")\n",
    "print(f\"   x[:, 0, 0] \u2192 ({B}, {D}) \u2192 norm \u2192 head(768, num_classes) \u2192 logits\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Transfer Learning: ImageNet ViT \u2192 TimeSformer\n",
    "\n",
    "The key insight: **spatial attention in TimeSformer is exactly the same operation as ViT's self-attention**, just applied per-frame. So we can directly copy ViT weights into the spatial attention + MLP.\n",
    "\n",
    "The **temporal attention** is new (ViT has no concept of time) \u2014 it's initialized randomly and must learn from video data.\n",
    "\n",
    "```\n",
    "ViT-Base (ImageNet)              TimeSformer\n",
    "\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550              \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "patch_embed.proj    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  patch_embed (Conv2d)\n",
    "                                 \n",
    "For each of 12 blocks:\n",
    "  attn.qkv          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  spatial_attn.in_proj\n",
    "  attn.proj          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  spatial_attn.out_proj\n",
    "  mlp.fc1            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  mlp[0] (Linear)\n",
    "  mlp.fc2            \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  mlp[2] (Linear)\n",
    "                                 \n",
    "  (nothing)          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192  temporal_attn (random init)\n",
    "```\n",
    "\n",
    "This gives the model strong spatial understanding from day one \u2014 it only needs to learn temporal dynamics during fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# pretrained vit\n",
    "\n",
    "vit = timm.create_model('vit_base_patch16_224', pretrained = True)\n",
    "\n",
    "vit.eval()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1o1fnF9gLCbq",
    "outputId": "9d0a6264-ae4d-4648-e6f1-37613877de57"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Loading Function\n",
    "\n",
    "Maps ViT's attention QKV weights (packed as a single matrix) to TimeSformer's `nn.MultiheadAttention` `in_proj_weight`.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def load_vit_weights_into_timesformer(timesformer, vit):\n",
    "    # Patch embedding\n",
    "    timesformer.patch_embed.weight.data.copy_(\n",
    "        vit.patch_embed.proj.weight.data\n",
    "    )\n",
    "    timesformer.patch_embed.bias.data.copy_(\n",
    "        vit.patch_embed.proj.bias.data\n",
    "    )\n",
    "\n",
    "    # Transformer blocks (spatial parts only)\n",
    "    for ts_block, vit_block in zip(timesformer.blocks, vit.blocks):\n",
    "        ts_block.spatial_attn.in_proj_weight.data.copy_(\n",
    "            vit_block.attn.qkv.weight.data\n",
    "        )\n",
    "        ts_block.spatial_attn.in_proj_bias.data.copy_(\n",
    "            vit_block.attn.qkv.bias.data\n",
    "        )\n",
    "        ts_block.spatial_attn.out_proj.weight.data.copy_(\n",
    "            vit_block.attn.proj.weight.data\n",
    "        )\n",
    "        ts_block.spatial_attn.out_proj.bias.data.copy_(\n",
    "            vit_block.attn.proj.bias.data\n",
    "        )\n",
    "\n",
    "        ts_block.mlp[0].weight.data.copy_(\n",
    "            vit_block.mlp.fc1.weight.data\n",
    "        )\n",
    "        ts_block.mlp[0].bias.data.copy_(\n",
    "            vit_block.mlp.fc1.bias.data\n",
    "        )\n",
    "        ts_block.mlp[2].weight.data.copy_(\n",
    "            vit_block.mlp.fc2.weight.data\n",
    "        )\n",
    "        ts_block.mlp[2].bias.data.copy_(\n",
    "            vit_block.mlp.fc2.bias.data\n",
    "        )\n",
    "\n",
    "    print(\"Loaded ImageNet ViT weights into TimeSformer (spatial only)\")"
   ],
   "metadata": {
    "id": "Vp-gX-3iXoRm"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Instantiate & Initialize\n",
    "\n",
    "ViT-Base config: 12 layers, 12 heads, 768-dim, 16\u00d716 patches \u2192 **N = (224/16)\u00b2 = 196** patches per frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = TimeSformer(\n",
    "    num_classes=2,\n",
    "    num_frames=8,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    heads=12,\n",
    ").to(device)\n",
    "\n",
    "load_vit_weights_into_timesformer(model, vit)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxfhEVtrXtO0",
    "outputId": "ddc8e0d7-ea8c-4324-d507-035bc99588aa"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded ImageNet ViT weights into TimeSformer (spatial only)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Dataset & DataLoader\n",
    "\n",
    "Point `root_dir` to your video frames directory. Use `batch_size=2` because TimeSformer is memory-hungry (12 blocks \u00d7 8 frames \u00d7 196 patches).\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# train_dataset = VideoFrameDataset(\n",
    "#     root_dir=\" ______ \",\n",
    "#     num_frames=8\n",
    "# )\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=2,\n",
    "#     shuffle=True,\n",
    "#     num_workers=2\n",
    "# )"
   ],
   "metadata": {
    "collapsed": true,
    "id": "eKbSMYJwYVFC"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Training\n",
    "\n",
    "Uses `AdamW` with a low learning rate (`1e-5`) since we're fine-tuning from pretrained spatial weights. The temporal attention learns from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, loader):\n",
    "  model.train()\n",
    "  total_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  for videos, labels in loader:\n",
    "      videos = videos.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      logits = model(videos)\n",
    "      loss = criterion(logits, labels)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      preds = torch.argmax(logits, dim=1)\n",
    "      correct += (preds == labels).sum().item()\n",
    "      total += labels.size(0)\n",
    "\n",
    "  avg_loss = total_loss / len(loader)\n",
    "  accuracy = correct / total\n",
    "\n",
    "  return avg_loss, accuracy\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "3YDX-yl4Y7u5"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# epochs = 10\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     train_loss, train_acc = train_one_epoch(model, train_loader)\n",
    "\n",
    "#     print(\n",
    "#         f\"Epoch {epoch+1} | \"\n",
    "#         f\"Train loss: {train_loss:.4f} | \"\n",
    "#         f\"Train accuracy: {train_acc:.4f}\"\n",
    "#     )"
   ],
   "metadata": {
    "id": "7MX94nHzbzeu"
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A complete TimeSformer that:\n",
    "1. **Patches** video frames using Conv2d (same as ViT)\n",
    "2. **Encodes position** with factorized time + space embeddings\n",
    "3. **Attends temporally** (same patch across frames \u2014 captures motion)\n",
    "4. **Attends spatially** (all patches within a frame \u2014 captures appearance)\n",
    "5. **Classifies** via CLS token aggregation\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The `einops.rearrange` trick makes factorized attention trivial to implement\n",
    "- Divided attention achieves **best accuracy with much lower cost** than joint attention\n",
    "- Transfer learning from ViT (spatial weights) enables strong performance with limited video data\n",
    "- The CLS token at `x[:, 0, 0]` aggregates information from all frames and all patches through the attention blocks\n"
   ]
  }
 ]
}